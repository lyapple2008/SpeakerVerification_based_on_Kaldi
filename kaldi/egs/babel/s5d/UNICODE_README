Graphemic Lexicon from Unicode
================================================================================

General Description
----------------------------------------
Given some form of word list in an unknown language, we must find pronunciations
for each word. When the language is written alphabetically, the letters
themselves can be used as word pronunciations. In English for instance there
would be 26 phones, and possibly a few extra for the rarely occuring letters,

  "ö","é","è","â", ...

which occur primarily in foreign loan words.

Some languages use syllabic systems or partially alphabetic scripts, for which
nothing close to a 1-1 mapping from graphemes to phonemes exists. Examples of
such are Abougidas and Abjads.

The premise of this system is that for most languages, there exists a unicode
description of the graphemes from which the phonetics may be recovered.

While non-alphabetic scripts present an obvious challenge, we find that even
for languages such as English and French, the issue of whether or not to treat
each accented character as a separate phone presents a problem. After all,
pâté, pâte, and pate are all English words with different pronunciations.
Resume, and résumé, are also examples. And this for a language that is generally
considered unaccented. In French, which is known to have many diacritics
affecting pronunciation, we nonetheless find words such as forêt, and bosquet,
with essentially the same meaning whose "e" sounds have very much the same
pronunciation. In some scripts, such diacritics are vowel markers, indicators
of tone, or stress, and probably many other linguistic phenomena we have not
yet encounted.

Fortunately, the unicode representation of such graphemes has an alternate
normalization, "NFD", which decomposes a grapheme into its constituent parts.
In this implementation we treat such marks as modifying the preceding grapheme.
When the grapheme occurs frequently enough, the accented grapheme is
automatically considered a separate phoneme. For infrequent accented graphemes
we treat the accent as a tag and use the tag as an extra question in the tree
building step.

The issue of syllable boundaries in words is mostly important for keyword-seach.
Syllables can be created by training a morphological analyser on the
conversational transcripts, and then segmenting each word into its learned
morphemes.

Usage
----------------------------------------
All the scripts for creating the graphemic lexicon are located in local/lexion,
except for prepare_unicode_lexicon.py. Run ...

./run-1-main-unicode.sh --unicode-lexicon true --morfessor true

for a full system run using a unicode-lexicon and morfessor.

The general structure is.

1. Generate list of unqiue words in the training data. Just use the word
   entries of the filtered_lexicon if available. Do not include words present in
   in conversation transcriptions such as <v-noise> <unk>, etc..

local/lexicon/phone2morph_lexicon.py

2. Use morfessor to create somewhat logical syllabic units. Train the system
   on the conversational transcriptions for instance, though any body of text
   in the language should do. The conversational transcriptions were used in
   this script however.

3. Segment each word in the word list into its morphemes. Represent this as
   a lexicon of sorts.

local/lexicon/make_unicode_lexicon.py

4. Use the morphemic lexicon created in step 3. as input.

5. Get the unicode representation for each grapheme in each word

local/lexicon/methods/blind_tags_counts.py

6. Convert the unicode representation of each word into actual units with
   which we derive an entry in the lexicon. This function is actually imported
   into make_unicode_lexicon.py It's written this way to allow for more
   flexibility in processing the unicode descriptions of graphemes.

local/prepare_unicode_lexicon.py
7. This creates the rest of the data/local directory. It also adds the extra
   questions derived from the unicode-derived tags to extra_questions.txt.


Script Descriptions
------------------------------------------------------------------------------
In local/lexicon,
make_unicode_lexicon.py <input_words> <output_lexicon> <directory_of_methods> <name_of_method>:

  This script takes as arguments: a lexicon, word-list, or file with distinct
  space separated words; a path to an output lexicon that will be generated; a
  directory containing all possible methods of processing the unicode
  character descriptions; and the name of the method in the directory to use.
  Options exist for specifying the type of input file, whether to treat the
  input lexicon entries as morphemes, etc..

In local/lexicon/methods
blind_tags_counts.py

  Each method in the methods directory is supposed to follow a strict format:
    1. Must have a fmt global specifying the output lexicon format
       (normally kalid).
    2. Must have an encode function which maps a certain structure in which
       unicode character descriptions were stored to lexicon entries in the
       new lexicon we are creating.
    3. Certain input arguments, especially a table argument for the table
       containing the mapping between unicode graphemes, and lexical entries.


In local/lexicon/methods
phone2morph_lexicon.py <word_list> <morphemic_dictionary>

    This script takes an input word list, and outputs a morphemic dictionary.

